# -*- coding: utf-8 -*-
"""model.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1NVc9G_4TwFTeYaCG3pSmu595SYJh2Hk-
"""

TIME_STEPS=30

def create_sequences(X, y, time_steps=TIME_STEPS):
    Xs, ys = [], []
    for i in range(len(X)-time_steps):
        Xs.append(X.iloc[i:(i+time_steps)].values)
        ys.append(y.iloc[i+time_steps])

    return np.array(Xs), np.array(ys)

X_train, y_train = create_sequences(train[['feature']], train['ground_truth'])
X_test, y_test = create_sequences(test[['feature']], test['ground_truth'])

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense

# Define number of features in your input data and number of units in LSTM layers
n_features = 20  # Replace with the number of features in your data
n_units_lstm1 = 128  # Replace with the desired number of units in 1st LSTM layer
n_units_lstm2 = 64  # Replace with the desired number of units in 2nd LSTM layer (optional)
n_units_lstm3 = 32
# Create the model
model = Sequential()

# Input layer
model.add(LSTM(n_units_lstm1, return_sequences=True, input_shape=(timesteps, n_features)))  # Adjust timesteps for sequence length

# Optional additional LSTM layer
model.add(LSTM(n_units_lstm2, return_sequences=False))  # Set return_sequences to False for last layer

# Optional additional LSTM layer
model.add(LSTM(n_units_lstm3, return_sequences=False))  # Set return_sequences to False for last layer
model.add(Activation('relu'))
# Final dense layer (output layer)
output_size =5 #Driving behavior label 1-5
model.add(Dense(output_size))  # Replace output_size with the number of output classes/dimensions
model.add(Activation('softmax'))
# Compile the model
model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])

# Train the model (replace with your training data)
model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size)